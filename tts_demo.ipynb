{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/interspeech2019-tutorial/blob/kan-bayashi/tts/tts_demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wA10eXyHIv8X",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ESPnet Text-to-Speech Demonstration\n",
    "\n",
    "[**Tomoki Hayashi**](https://github.com/kan-bayashi)\n",
    "\n",
    "Department of infomatics, Nagoya University  \n",
    "Human Dataware Lab. Co., Ltd.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8xWr9FADn0Rf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup envrionment\n",
    " \n",
    "It take around 10 minues. Please keep waiting for a while.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J6k0hPgzjxA5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OS setup\n",
    "!apt-get install -qq bc tree\n",
    "!cat /etc/os-release\n",
    "\n",
    "# espnet setup\n",
    "!git clone https://github.com/espnet/espnet\n",
    "!pip install -q torch==1.1\n",
    "!cd espnet; pip install -q -e .\n",
    "\n",
    "# warp ctc setup\n",
    "!git clone https://github.com/espnet/warp-ctc -b pytorch-1.1\n",
    "!cd warp-ctc && mkdir build && cd build && cmake .. && make -j\n",
    "!cd warp-ctc/pytorch_binding && python setup.py install \n",
    "\n",
    "# kaldi setup\n",
    "!cd espnet/tools && git clone https://github.com/kaldi-asr/kaldi\n",
    "!echo \"\" > ./espnet/tools/kaldi/tools/extras/check_dependencies.sh # ignore check\n",
    "!chmod +x ./espnet/tools/kaldi/tools/extras/check_dependencies.sh\n",
    "!cd ./espnet/tools/kaldi/tools; make sph2pipe sclite\n",
    "!rm -rf espnet/tools/kaldi/tools/python\n",
    "!espnet/utils/download_from_google_drive.sh https://drive.google.com/open?id=1DW4zKQtgDt-YeImLE_kS1Ldj673x7Sx2 downloads tar.gz\n",
    "!mkdir -p espnet/tools/kaldi/src/featbin && mv downloads/featbin/* espnet/tools/kaldi/src/featbin/\n",
    "\n",
    "# make dummy activate\n",
    "!mkdir -p espnet/tools/venv/bin && touch espnet/tools/venv/bin/activate\n",
    "!echo \"setup done.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction of ESPnet TTS\n",
    "\n",
    "- Follow the [Kaldi](https://github.com/kaldi-asr/kaldi) recipe style\n",
    "- Multi GPU training / GPU decoding thanks to Pytorch\n",
    "- Support three E2E-TTS models and their variants\n",
    "- Support four corpus\n",
    "- Support additional attention mechanisms and loss functions\n",
    "- Support pretrained WaveNet-vocoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supported E2E-TTS models\n",
    "\n",
    "- [**Tacotron 2**](https://arxiv.org/abs/1712.05884): Standard Tacontron 2\n",
    "- [**Multi-speaker Tacotron2**](https://arxiv.org/pdf/1806.04558.pdf): Pretrained x-vector + Tacotron 2\n",
    "- [**Transformer**](https://arxiv.org/pdf/1809.08895.pdf): TTS-Transformer\n",
    "- [**Multi-speaker Transformer**](): Pretrained x-vector + TTS-Transformer\n",
    "- [**FastSpeech**](https://arxiv.org/pdf/1905.09263.pdf): Feed-forward TTS-Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other remarkable functions\n",
    "\n",
    "- [**CBHG**](https://arxiv.org/pdf/1703.10135.pdf): Network to convert Mel-filter bank to linear spectrogram\n",
    "- [**Forward attention**](https://arxiv.org/pdf/1807.06736.pdf): Attention mechanism with causal regularization\n",
    "- [**Guided attention loss**](https://arxiv.org/pdf/1710.08969.pdf): Loss function to force attention to be diagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supported corpus\n",
    "\n",
    "- [`egs/jsut/tts1`](https://sites.google.com/site/shinnosuketakamichi/publication/jsut): Japanese single female speaker. (48 kHz, ~10 hours)\n",
    "- [`egs/libritts/tts1`](http://www.openslr.org/60/): English multi sepaker (24 kHz, ~500 hours).\n",
    "- [`egs/ljspeech/tts1`](https://keithito.com/LJ-Speech-Dataset/): English single female speaker (22.05 kHz, ~24 hours).\n",
    "- [`egs/m_ailabs/tts1`](https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/): Various language speakers (16 kHz, 16~48 hours)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QhW6ey37Kj7y",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Run the recipe\n",
    " \n",
    "Use the most simplest recipe `egs/an4/tts1` as an example.  \n",
    "\n",
    "Unfortunately, `egs/an4/tts1` is too small to train.   \n",
    "But the flow itself is the same as the other recipes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z2G_kA8VpM8X"
   },
   "outputs": [],
   "source": [
    "# Let's go to an4 recipe!\n",
    "import os\n",
    "os.chdir(\"espnet/egs/an4/tts1\")\n",
    "!echo $(pwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kK-qFOFupi-I",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scripts in the recipe\n",
    "\n",
    "- `run.sh`: Main script of the recipe.\n",
    "- `cmd.sh`: Command configuration script to control how-to-run each job.\n",
    "- `path.sh`: Path configuration script. Basically, we do not have to touch.\n",
    "- `conf/`: Directory containing configuration files.\n",
    "- `local/`: Directory containing the recipe-specific scripts e.g. data preparation.\n",
    "- `steps/` and `utils/`: Directory containing kaldi tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "X6T2rNlSpVMV",
    "outputId": "2cc5b211-b004-43b8-afae-d15672464f90"
   },
   "outputs": [],
   "source": [
    "!tree -L 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overview of the recipe\n",
    "\n",
    "<img src=figs/tts_overview.png width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aGX_y4RMpqIK",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stages in the recipe\n",
    "\n",
    "Main script **run.sh** consists of several stages:\n",
    "\n",
    "- **stage -1**: Download data if the data is available online.\n",
    "- **stage 0**: Prepare data to make kaldi-stype data directory.\n",
    "- **stage 1**: Extract feature vector, calculate statistics, and perform normalization.\n",
    "- **stage 2**: Prepare a dictionary and make json files for training.\n",
    "- **stage 3**: Train the E2E-TTS network.\n",
    "- **stage 4**: Decode mel-spectrogram using the trained network.\n",
    "- **stage 5**: Generate a waveform using Griffin-Lim.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l6Zhc4iKLKeC",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage -1: Data download\n",
    "\n",
    "<img src=figs/tts_stage-1.png width=80%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "8CNFWXETAeYC",
    "outputId": "e6559ffa-c534-4cfe-9295-f5a82924f8d7",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage -1 and then stop\n",
    "!./run.sh --stage -1 --stop_stage -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lzqGzOqpw741",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`downloads` directory is cretead, which containing donwloaded an4 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "Y3FTgWO7w0Rf",
    "outputId": "250de278-54c6-4e28-fd85-64d8f84c2942",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# check directroy structure\n",
    "!tree -L 1\n",
    "\n",
    "# check downloads directory\n",
    "!ls downloads/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wa1OkxN3xSRx",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 0: Data preparation\n",
    "\n",
    "<img src=figs/tts_stage0.png width=80%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "ozLaRF1LA3Aq",
    "outputId": "5ac935b7-2fc7-415c-ccff-1b45e308adab",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage 0 and then stop\n",
    "!./run.sh --stage 0 --stop_stage 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bF6wu5n3x9gD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Two kaldi-style data directories are created:  \n",
    "- `data/train`: data directory of training set\n",
    "- `data/test`: data directory of evaluation set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "5Ch0fonjxep5",
    "outputId": "48bc5bd7-80e5-4794-9925-ae67c3fc51cb",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# check directory structure\n",
    "!tree -L 1 data\n",
    "\n",
    "# check each directory\n",
    "!ls data/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EwPydMzvxbcA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`wav.scp`: \n",
    "- Each line has `<utt_id> <wavfile_path or command pipe>`\n",
    "- `<utt_id>` must be unique\n",
    "\n",
    "`text`:\n",
    "- Each line has `<utt_id> <transcription>`\n",
    "- Assume that `<transcription>` is cleaned\n",
    "\n",
    "`utt2spk`:\n",
    "- Each line has `<utt_id> <speaker_id>`\n",
    "\n",
    "`spk2utt`:\n",
    "- Each lien has `<speaker_id> <utt_id> ... <utt_id> `\n",
    "- Can be automatically created from `utt2spk` \n",
    "\n",
    "In the ESPnet, speaker information is not used for any processing.   \n",
    "Therefore, **utt2spk** and **spk2utt** can be a dummy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "S5PJH9zxyqNg",
    "outputId": "88f99c71-9309-494c-ec36-82e3fd29c93d"
   },
   "outputs": [],
   "source": [
    "# check each file\n",
    "!head -n 3 data/train/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XEgfec6u1KWA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 1: Feature extration\n",
    "\n",
    "<img src=figs/tts_stage1.png width=80%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters related to stage 1\n",
    "!head -n 28 run.sh | tail -n 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "znHW0IwbBX0o",
    "outputId": "957f9dce-c181-433a-812b-3c79d2933653",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage 1 with default settings\n",
    "!./run.sh --stage 1 --stop_stage 1 --nj 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check directory structure\n",
    "!tree -L 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e9OS6JIp474Z",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Raw filterbanks are saved in `fbank/` directory with `ark/scp` format.\n",
    "\n",
    "- `.ark`: binary file of faeture vector\n",
    "- `.scp`: list of the correspondance b/w `<utt_id>` and `<path_in_ark>`.  \n",
    "\n",
    "Since feature extraction can be performed for split small sets in parallel, raw_fbank is split into `raw_fbank_*.{1..4}.{scp,ark}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "udo85LqA19Yq",
    "outputId": "73535451-9620-4439-ad63-4d498608e01e"
   },
   "outputs": [],
   "source": [
    "!ls fbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "tuPfH0ly46Ko",
    "outputId": "76de2cad-96b9-4781-afd3-7509960de58c"
   },
   "outputs": [],
   "source": [
    "!head -n 3 fbank/raw_fbank_train.1.scp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oj1bx25q5bl_",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These files can be loaded in python via **kaldiio** as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "3K0uiQgB5aYK",
    "outputId": "d7797bdd-820b-45e4-ca91-ea821d5e7b36"
   },
   "outputs": [],
   "source": [
    "import kaldiio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load scp file\n",
    "scp_dict = kaldiio.load_scp(\"fbank/raw_fbank_train.1.scp\")\n",
    "for key in scp_dict:\n",
    "    plt.imshow(scp_dict[key].T[::-1])\n",
    "    plt.title(key)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    break\n",
    "    \n",
    "# load ark file\n",
    "ark_generator = kaldiio.load_ark(\"fbank/raw_fbank_train.1.ark\")\n",
    "for key, array in ark_generator:\n",
    "    plt.imshow(array.T[::-1])\n",
    "    plt.title(key)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Au70T7i8ctz",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some files are added in `data/train`  \n",
    "- `feats.scp`: concatenated scp file of `fbank/raw_fbank_train.{1..4}.scp`.  \n",
    "- `utt2num_frames`: Each line has `<utt_id> <number_of_frames>` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "ySykrx407Tek",
    "outputId": "a0829cb1-f291-4d51-cd17-e7da04ec3d53"
   },
   "outputs": [],
   "source": [
    "!ls data/train\n",
    "!head -n 3 data/train/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qt4s7lkl9miZ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And `data/train/` directory is split into two directory:\n",
    "- `data/train_nodev/`: data directory for training\n",
    "- `data/train_dev/`: data directory for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "Yq6WNznw9P4z",
    "outputId": "be2c96cc-c9fb-4068-b1d0-3367a15cbb8f"
   },
   "outputs": [],
   "source": [
    "!ls data\n",
    "!ls data/train_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fw_KBzQCBN0E",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "`cmvn.ark` is saved in `data/train_nodev`, which is the statistics file.  \n",
    "This file also can be loaded in python via kaldiio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "colab_type": "code",
    "id": "s5YxfvNwBIlG",
    "outputId": "76ce0685-53e6-4ff3-9195-8dcdeb35b3cf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import kaldiio\n",
    "\n",
    "# load cmvn.ark file (Be careful not load_ark, but load_mat)\n",
    "cmvn = kaldiio.load_mat(\"data/train_nodev/cmvn.ark\")\n",
    "\n",
    "# cmvn consists of mean and variance, the last dimension of mean represents the number of frames.\n",
    "print(\"cmvn shape = \"+ str(cmvn.shape))\n",
    "\n",
    "# calculate mean and variance\n",
    "mu = cmvn[0, :-1] / cmvn[0, -1]\n",
    "var = cmvn[1, :-1] / cmvn[0, -1]\n",
    "\n",
    "# show mean\n",
    "print(\"mean = \" + str(mu))\n",
    "print(\"variance = \" + str(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTCG1CPRDN5H",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Normalzed features for train, dev, and eval sets are dumped in\n",
    "- `dump/{train_nodev,train_dev,test}/*.{ark,scp}`.  \n",
    "\n",
    "There ark and scp can be loaded as the same as the above procedure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "JazwdIVbDNFs",
    "outputId": "0ac7485e-7ea7-4519-a7e4-347d041e9389"
   },
   "outputs": [],
   "source": [
    "!ls dump/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eYtmQCDdDk0d",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 2: Dictionary and json preparation\n",
    "\n",
    "<img src=figs/tts_stage2.png width=80%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "fgWDdveUBctQ",
    "outputId": "1f32337d-5e62-4462-d908-224b277ee647",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage 2 and then stop\n",
    "!./run.sh --stage 2 --stop_stage 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IB-eU-W1FNLk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Dictrionary file will be created in `data/lang_1char/`.  \n",
    "- Dictionary file consists of `<token>` `<token index>`.  \n",
    "    - `<token index>` starts from 1 because 0 is used as padding index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "colab_type": "code",
    "id": "q8S74SjdFXcF",
    "outputId": "02f0fd30-a05f-4804-b6a8-5938782269d6"
   },
   "outputs": [],
   "source": [
    "!ls data\n",
    "!cat data/lang_1char/train_nodev_units.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b-QO0MLGGaBO",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Json file will be created for train, dev, and eval sets as \n",
    "- `dump/{train_nodev,train_dev,test}/data.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BrYKpYVUGh6K",
    "outputId": "1f168c10-6407-411e-b617-eafc5b1dae24"
   },
   "outputs": [],
   "source": [
    "!ls dump/*/*.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CoRMOHDnG1Xk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each json file contains all of the information in the data directory.\n",
    "\n",
    "- `shape`: Shape of the input or output sequence. [63, 80] represents the number of frames = 63 and the dimension of mel-spectrogram = 80.\n",
    "- `text`: Original transcription.\n",
    "- `token`: Token sequence of original transcription.\n",
    "- `tokenid` Token id sequence of original transcription, which is converted using the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "O_My36NFHAVI",
    "outputId": "02642af2-6ae1-4c64-ab74-3b9ee2ada422"
   },
   "outputs": [],
   "source": [
    "!head -n 27 dump/train_nodev/data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O88CpXHtHnZA"
   },
   "source": [
    "Now ready to start training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QUkLphZbIw51",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 3: Network training\n",
    "\n",
    "<img src=figs/tts_stage3.png width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Training setting can be specified by `train_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "colab_type": "code",
    "id": "b-f5K2bYOaVY",
    "outputId": "22c2ec06-6a8a-4440-8ab3-e3eb877fdb68"
   },
   "outputs": [],
   "source": [
    "# check hyperparmeters in run.sh\n",
    "!head -n 31 run.sh | tail -n 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training configurations are written as `.yaml` format file.  \n",
    "Let us check the default cofiguration `conf/train_pytroch_tacotron2.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!cat conf/train_pytorch_tacotron2.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6_QmCAS1OlD2",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's change the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "colab_type": "code",
    "id": "f2Q9sYVxOjzm",
    "outputId": "e32d1ff5-dd58-41b0-8475-0eeb8ca65f16"
   },
   "outputs": [],
   "source": [
    "# load configuration yaml\n",
    "import yaml\n",
    "with open(\"conf/train_pytorch_tacotron2.yaml\") as f:\n",
    "    params = yaml.load(f, Loader=yaml.Loader)\n",
    "\n",
    "# change hyperparameters by yourself!\n",
    "params = {\n",
    "    \"embed-dim\": 16,\n",
    "    \"elayers\": 1,\n",
    "    \"eunits\": 16,\n",
    "    \"econv-layers\": 1,\n",
    "    \"econv-chans\": 16,\n",
    "    \"econv-filts\": 5,\n",
    "    \"dlayers\": 1,\n",
    "    \"dunits\": 16,\n",
    "    \"prenet-layers\": 1,\n",
    "    \"prenet-units\": 16,\n",
    "    \"postnet-layers\": 1,\n",
    "    \"postnet-chans\": 16,\n",
    "    \"postnet-filts\": 5,\n",
    "    \"atype\": 16,\n",
    "    \"adim\": 16,\n",
    "    \"aconv-chans\": 16,\n",
    "    \"aconv-filts\": 5,\n",
    "    \"reduction-factor\": 3,\n",
    "    \"batch-size\": 64,\n",
    "    \"epochs\": 10,\n",
    "}\n",
    "\n",
    "# save\n",
    "with open(\"conf/train_pytorch_tacotron2_mini.yaml\", \"w\") as f:\n",
    "    yaml.dump(params, f, Dumper=yaml.Dumper)\n",
    "\n",
    "!cat conf/train_pytorch_tacotron2_mini.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MlkD_x4wRD80",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's train the network.  \n",
    "You can specify the config file via `--train_config` option.  \n",
    "It takes several minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "cgdY6vp0Bhy4",
    "outputId": "73a6fc3a-5e44-4dd7-c229-42037276ba44",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# use modified configuration file as train config\n",
    "!./run.sh --stage 3 --stop_stage 3 --train_config conf/train_pytorch_tacotron2_mini.yaml --verbose 1 --ngpu 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HsN8qD1gKnTc",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can see the training log in `exp/train_*/train.log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat exp/train_nodev_pytorch_train_pytorch_tacotron2_mini/train.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wy0rXqBNTAtk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The models are saved in `exp/train_*/results/` directory.\n",
    "\n",
    "- `exp/train_*/results/model.loss.best`: contains only the model parameters.  \n",
    "- `exp/train_*/results/snapshot.ep.*`: contains the model parameters, optimizer states, and iterator states. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "YRxIFjnWS_t9",
    "outputId": "5408227c-5118-4b6f-f36f-e6a72e3ddc58"
   },
   "outputs": [],
   "source": [
    "!ls exp/train_nodev_pytorch_train_pytorch_tacotron2_mini/{results,results/att_ws}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_vyKmHl8T5uj",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`exp/train_*/results/*.png` are the figures of training curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Dwee4a8NUOBg",
    "outputId": "ea2a6c3f-4296-4ab7-f918-2d378227aec8"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display_png\n",
    "print(\"all loss curve\")\n",
    "display_png(Image(\"exp/train_nodev_pytorch_train_pytorch_tacotron2_mini/results/all_loss.png\"))\n",
    "print(\"l1 loss curve\")\n",
    "display_png(Image(\"exp/train_nodev_pytorch_train_pytorch_tacotron2_mini/results/l1_loss.png\"))\n",
    "print(\"mse loss curve\")\n",
    "display_png(Image(\"exp/train_nodev_pytorch_train_pytorch_tacotron2_mini/results/mse_loss.png\"))\n",
    "print(\"bce loss curve\")\n",
    "display_png(Image(\"exp/train_nodev_pytorch_train_pytorch_tacotron2_mini/results/bce_loss.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ui57IrR4VCpv",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`exp/train_*/results/att_ws/.png` are the figures of attention weights in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "colab_type": "code",
    "id": "NyMW2tx0VKjC",
    "outputId": "d301ec6f-1a0a-4aa5-efb7-c69fd978d4ae"
   },
   "outputs": [],
   "source": [
    "print(\"Attention weights of initial epoch\")\n",
    "display_png(Image(\"exp/train_nodev_pytorch_train_pytorch_tacotron2_mini/results/att_ws/fash-cen1-b.ep.1.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s6_hIi3OVsvZ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " You can restart from the training by specifying the snapshot file with `--resume` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "bYY323h4WVgQ",
    "outputId": "2767c24d-3d0e-41e8-9cca-147b77813266"
   },
   "outputs": [],
   "source": [
    "# resume training from snapshot.ep.2\n",
    "!./run.sh --stage 3 --stop_stage 3 --verbose 1 \\\n",
    "    --train_config conf/train_pytorch_tacotron2_mini.yaml \\\n",
    "    --resume exp/train_nodev_pytorch_train_pytorch_tacotron2_sample/results/snapshot.ep.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JNfzDyQRWsf-"
   },
   "outputs": [],
   "source": [
    "!cat exp/train_nodev_pytorch_train_pytorch_tacotron2_mini/train.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JjkrYF-NeZoM",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Also, we support tensorboard.  \n",
    "You can see the training log through tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OqwnbGznmrgb"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tensorboard/train_nodev_pytorch_train_pytorch_tacotron2_mini/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "88UvHDgGJSLX",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 4: Network decoding\n",
    "\n",
    "<img src=figs/tts_stage4.png width=80%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Decoding parameters can be specified by `--decode_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 32 run.sh | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding configurations are written as `.yaml` format file.  \n",
    "Let us check the default cofiguration `conf/decode.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat conf/decode.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PjVP2QPXBmav",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage 4 and then stop\n",
    "!./run.sh --stage 4 --stop_stage 4 --nj 8 --train_config conf/train_pytorch_tacotron2_mini.yaml "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I7-8ja9kcdDG",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Generated features are saved as `ark/scp` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8V_g_VbDckjT"
   },
   "outputs": [],
   "source": [
    "!ls exp/train_nodev_pytorch_train_pytorch_tacotron2_mini/outputs_model.loss.best_decode/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eLvdP4z9cJKc",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can specify the model or snapshot for decoding via `--model`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Op5GAW6cM7q"
   },
   "outputs": [],
   "source": [
    "!./run.sh --stage 4 --stop_stage 4 --nj 8 --train_config conf/train_pytorch_tacotron2_mini.yaml --model snapshot.ep.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hFowPMOsdvY9"
   },
   "outputs": [],
   "source": [
    "!ls exp/train_nodev_pytorch_train_pytorch_tacotron2_mini/outputs_snapshot.ep.2_decode/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9A74Jv8KL_FC",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stage 5: Waveform synthesis\n",
    "\n",
    "<img src=figs/tts_stage5.png width=80%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wasoVKklc4eS",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# run stage 5 and then stop\n",
    "!./run.sh --stage 5 --stop_stage 5 --nj 8 --train_config conf/train_pytorch_tacotron2_mini.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E7eoOkRYeIhN",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Generated wav files are saved in \n",
    "- `exp/train_nodev_pytorch_*/outputs_model.loss.best_decode_denorm/*/wav`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kp6LcFfih0hJ"
   },
   "outputs": [],
   "source": [
    "!tree -L 3 exp/train_nodev_pytorch_train_pytorch_tacotron2_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-pfSNMheHdf"
   },
   "outputs": [],
   "source": [
    "!ls exp/train_nodev_pytorch_train_pytorch_tacotron2_mini/outputs_model.loss.best_decode_denorm/*/wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use pretrained models\n",
    "\n",
    "We provide pretrained models and these are easy to use them with `synth_wav.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move on directory\n",
    "os.chdir(\"../../librispeech/asr1\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check usage\n",
    "!../../../utils/synth_wav.sh --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# generate your sentence!\n",
    "print(\"Please input your favorite sentence!\")\n",
    "text = input()\n",
    "text = text.upper()\n",
    "with open(\"example.txt\", \"w\") as f:\n",
    "    f.write(text)\n",
    "!../../../utils/synth_wav.sh --models ljspeech.fastspeech.v1 example.txt\n",
    "\n",
    "# check generated audio\n",
    "import IPython.display\n",
    "IPython.display.display(IPython.display.Audio(\"decode/example/wav/example.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's recognize generated speech!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# check usage\n",
    "!../../../utils/recog_wav.sh --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# downsample to 16 kHz for ASR model\n",
    "!sox decode/example/wav/example.wav -b 16 decode/example/wav/example_16k.wav rate 16k\n",
    "\n",
    "# make decode config\n",
    "import yaml\n",
    "with open(\"conf/decode_sample.yaml\", \"w\") as f:\n",
    "    yaml.dump({\n",
    "        \"batchsize\": 0,\n",
    "        \"beam-size\": 5,\n",
    "        \"ctc-weight\": 0.4,\n",
    "        \"lm-weight\": 0.6,\n",
    "        \"maxlenratio\": 0.0,\n",
    "        \"minlenratio\": 0.0,\n",
    "        \"penalty\": 0.0,\n",
    "    }, f, Dumper=yaml.Dumper)\n",
    "\n",
    "# let's recognize generated speech\n",
    "!../../../utils/recog_wav.sh --models librispeech.transformer.v1 \\\n",
    "    --decode_config conf/decode_sample.yaml \\\n",
    "    decode/example/wav/example_16k.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next steps\n",
    "\n",
    "- Try other recipes\n",
    "- Make your own recipe\n",
    "- Add your original model architecture"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "tts_an4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
